1) While performing manual tests and monitoring automated test runs, whether there are any obvious visible UI errors seen or not, how can you monitor for issues, hidden or otherwise, on both the client and server side?

When perform manual testing:
1. Using browser console/network manager
2. Using server logs (ssh to server and tail server logs) directly or through 3rd party tools such as Sentry, Papertrail

When perform automated testing:
It is possible to catch browser console logs using selenium. As for serverside I think it is also possible to ssh to server automatically
and parse logs somehow

2) When testing a web app manually or via automation, how can you test for (client side) usability and performance?

As for usability testing I think that it is some kind of abstract testing. The obvious things which can be tested are broken links and UI in general. For example, menus, buttons or links to different pages on our site should be easily visible and consistent on all webpages, images if present should contain an "alt" text. If we have UI specification that we assume is correct, we can test UI using tools like Galen, gemini etc. 
Also there are many online crowdsourcing services to determine psychological perception or how user will behave on our site (https://usabilityhub.com etc.)

When testing usability we usually pay attenttion to:
- navigation (how an user surfs the web pages, different controls like buttons, boxes or how the user uses the links on the pages to surf different pages)
- website should be easy to use
- instructions provided must be clear
- main menu should be provided on each page
- content should be logical and easy to understand
- no spelling errors
- etc.

As for performance testing - the main puprpose is to determine the speed or effectiveness of the app. Thats why to do something manually is trivial case. To see how good our site we can use browser developer tools: network, timeline, profiles tabs. Also we can capture network activity with tools like fiddler or browsermob, that will provide us a .har file which also can be interpreted (similar to network tab in browser)
Tools such as jMeter are often used for such kind of testing. 

3) How do you reverse engineer AJAX calls and (REST) web service API calls made by a website or web application (which you canâ€™t see via the UI)?

Open browser console and check network tab. Also Postman can be used. There is a big variety of such tools.

4) What goes into the body of a good bug report?

1. Bug summary (title)
2. Bug severity (trivial, minor, major, critical, blocker)
3. Bug priority (p1, p2, p3)
4. Component (part of the system (payments, subscriptions etc) or backend/frontend)
5. Environment (Browser, Server)
6. Test data used: user (login/password) etc.
7. 

Steps to reproduce:

1. Open url ...
2. ...
....
n. ...

Actual result: ...
Expected result: ...

Attachments - screenshots, browser/server logs, .gif / .mov files

5) Could you describe the way how to run testing in the agile environment?

1. Testing begins on the requirements stage
2. Testeres receive requirements, analyze, write test cases. 
Automation team writes and supports their tests. Developers - theirs (unit, integration)
3. Developers write code
4. Once it is done - code pushed to VCS
5. Code review - if passed => go to 6, else => go to 3
6. Automated build to run unit/integration tests is triggered. If success => go to 7, else => go to 3
7. Acceptance tests execution triggered. If tests success => go to 8, else => go to 3.
8. Testers perform some manual, exploratory, capacity testing. 
9. Analyze test results, if acceptance criteria are met (for ex. no bugs of major severity and high priority) then => we have success build, else => provide feedback, go to 3.


Other cases are also possible. For example such teams perform big regression testing at the end of each iterations. 
